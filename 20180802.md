### 用基于注意力机制的seq2seq神经网络进行翻译:
http://pytorch.apachecn.org/cn/tutorials/intermediate/seq2seq_translation_tutorial.html

#### 编码器
seq2seq网络的编码器是一个RNN,它为输入句子中的每个单词输出一些值.
对于每个输入字,编码器输出一个向量和一个隐藏状态,并将隐藏状态用于下一个输入字.

![avatar](http://pytorch.apachecn.org/cn/tutorials/_images/encoder-network.png)

#### 简单的解码器
在解码的每一步,解码器都被赋予一个输入指令和隐藏状态. 初始输入指令字符串开始的``<SOS>``指令,第一个隐藏状态是上下文向量(编码器的最后隐藏状态).

![avatar](http://pytorch.apachecn.org/cn/tutorials/_images/decoder-network.png)

#### 注意力解码器
注意力允许解码器网络针对解码器自身输出的每一步”聚焦”编码器输出的不同部分.
首先我们计算一组注意力权重. 这些将被乘以编码器输出矢量获得加权的组合.
结果应该包含关于输入序列的特定部分的信息, 从而帮助解码器选择正确的输出单词.

使用解码器的输入和隐藏状态作为输入,利用另一个前馈层 ``attn``计算注意力权重,
由于训练数据中有各种大小的句子,为了实际创建和训练此层,
我们必须选择最大长度的句子(输入长度,用于编码器输出),以适用于此层.
最大长度的句子将使用所有注意力权重,而较短的句子只使用前几个.

![avatar](http://pytorch.apachecn.org/cn/tutorials/_images/attention-decoder-network.png)


