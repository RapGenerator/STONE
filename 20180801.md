### Word2Vec:

one-hot问题：稀疏，任何两个词之间的余弦相似度为0。

word2vec：
* 跳字模型：skip-gram
* 连续词袋模型：CBOW（continuous bag of words）

两种高效的训练方法：
* 负采样
* 层序softmax

#### skip-gram:中心词，背景词，时间窗

最大化联合概率：中心词周围的背景词的概率

计算量大：每一次的时间复杂度为O(|V|),V:词典

#### CBOW：用一个中心词周围的词来预测该中心词

#### 负采样：O(|V|) --> O(k)
以skip-gram为例：
softmax考虑了背景词可能是词典中的任一词==>

假设中心词Wc生成背景词Wo由一下两个相互独立事件联合组成的近似：

* 中心词 wc 和背景词 wo 同时出现时间窗口。
* 中心词 wc 和第 1 个噪声词 w1 不同时出现在该时间窗口（噪声词 w1 按噪声词分布 ℙ(w) 随机生成，且假设一定和 wc 不同时出现在该时间窗口）。
* …
* 中心词 wc 和第 K 个噪声词 wK 不同时出现在该时间窗口（噪声词 wK 按噪声词分布 ℙ(w) 随机生成，且假设一定和 wc 不同时出现在该时间窗口）。

#### 层序softmax：O(|V|) --> O(log(|V|))
二叉树:树的每个叶子节点代表着词典V中的每个词。

### GloVe
GloVe 使用了词与词之间的共现（co-occurrence）信息。

我们定义 X 为共现词频矩阵，其中元素 xij 为词 j 出现在词 i 的背景的次数。

共现概率比值

用词向量表达共现概率比值

### fastText
